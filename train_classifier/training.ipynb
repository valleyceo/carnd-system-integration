{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic Light Training\n",
    "---\n",
    "\n",
    "#### Resources: \n",
    "1. [Training on Google Cloud](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_pets.md)\n",
    "2. [Training the model (medium-WuStangDan)](https://medium.com/@WuStangDan/step-by-step-tensorflow-object-detection-api-tutorial-part-4-training-the-model-68a9e5d5a333)\n",
    "3. [Sample .config file](https://github.com/tensorflow/models/blob/master/research/object_detection/faster_rcnn_inception_resnet_v2_atrous_oid.config)\n",
    "4. [Reading TFRecord file](http://warmspringwinds.github.io/tensorflow/tf-slim/2016/12/21/tfrecords-guide/)\n",
    "5. [COCO Trained Models](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md)\n",
    "6. [Udacity Transfer Learning Lab](https://github.com/udacity/CarND-Transfer-Learning-Lab/blob/master/feature_extraction_solution.py)\n",
    "7. [Ronny Restrepo's transfer learning example](http://ronny.rest/blog/post_2017_10_13_tf_transfer_learning/)\n",
    "\n",
    "\n",
    "## Environment Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import skimage.io as io\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# This is needed to display the images.\n",
    "%matplotlib inline\n",
    "\n",
    "# Dataset paths\n",
    "PATH_TO_GREEN_IMGS = \"../../dataset/images_for_training/green/\"\n",
    "PATH_TO_RED_IMGS = \"../../dataset/images_for_training/red/\"\n",
    "PATH_TO_YELLOW_IMGS = \"../../dataset/images_for_training/yellow/\"\n",
    "PATH_TO_XML = \"../../dataset/images_for_training/\"\n",
    "OUTPUT_PATH = os.path.dirname(\"../../output_data/\")\n",
    "\n",
    "# Place TF Models repository on the same directory (https://github.com/tensorflow/models)\n",
    "sys.path.append(\"../../models/research/\")\n",
    "sys.path.append(\"../../models/research/slim/\") # deployment\n",
    "sys.path.append(\"../../models/research/object_detection/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check TF File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "data_path = '../../train_data/bosch_test.record'\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    feature = {'train/image': tf.FixedLenFeature([], tf.string),\n",
    "               'train/label': tf.FixedLenFeature([], tf.int64)}\n",
    "    # Create a list of filenames and pass it to a queue\n",
    "    filename_queue = tf.train.string_input_producer([data_path], num_epochs=1)\n",
    "    # Define a reader and read the next record\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    # Decode the record read by the reader\n",
    "    features = tf.parse_single_example(serialized_example, features=feature)\n",
    "    # Convert the image data from string back to the numbers\n",
    "    image = tf.decode_raw(features['train/image'], tf.float32)\n",
    "    \n",
    "    # Cast label data into int32\n",
    "    label = tf.cast(features['train/label'], tf.int32)\n",
    "    # Reshape image data into the original shape\n",
    "    image = tf.reshape(image, [224, 224, 3])\n",
    "    \n",
    "    # Any preprocessing here ...\n",
    "    array = image.eval(session=sess)\n",
    "    print(array)\n",
    "    # display encoded back to image data\n",
    "    #jpeg_bin = sess.run(image)\n",
    "    #image_np = sess.run(image)\n",
    "    #print(image_np.shape)\n",
    "    \n",
    "    #result = sess.run(image)\n",
    "    #plt.imshow(result)\n",
    "    #plt.show()\n",
    "\n",
    "    # Creates batches by randomly shuffling tensors\n",
    "    #images, labels = tf.train.shuffle_batch([image, label], batch_size=10, capacity=30, num_threads=1, min_after_dequeue=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Retrieve and Check TF records file ###\n",
    "tfrecords_filename = 'bosch_test.record'\n",
    "\n",
    "filename_queue = tf.train.string_input_producer(\n",
    "    [tfrecords_filename], num_epochs=10)\n",
    "\n",
    "# Even when reading in multiple threads, share the filename\n",
    "# queue.\n",
    "image, annotation = read_and_decode(filename_queue)\n",
    "\n",
    "# The op for initializing the variables.\n",
    "init_op = tf.group(tf.global_variables_initializer(),\n",
    "                   tf.local_variables_initializer())\n",
    "\n",
    "with tf.Session()  as sess:\n",
    "    \n",
    "    sess.run(init_op)\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "    # Let's read off 3 batches just for example\n",
    "    for i in range(1):\n",
    "    \n",
    "        img, anno = sess.run([image, annotation])\n",
    "        print(img[0, :, :, :].shape)\n",
    "        \n",
    "        print('current batch')\n",
    "        \n",
    "        # We selected the batch size of two\n",
    "        # So we should get two image pairs in each batch\n",
    "        # Let's make sure it is random\n",
    "\n",
    "        io.imshow(img[0, :, :, :])\n",
    "        io.show()\n",
    "\n",
    "        io.imshow(anno[0, :, :, 0])\n",
    "        io.show()\n",
    "        \n",
    "        io.imshow(img[1, :, :, :])\n",
    "        io.show()\n",
    "\n",
    "        io.imshow(anno[1, :, :, 0])\n",
    "        io.show()\n",
    "        \n",
    "    \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import COCO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pick and download a model\n",
    "\n",
    "model_name = 'faster_rcnn_resnet101_coco_2017_11_08'\n",
    "\n",
    "\n",
    "# Assume fc_2nd_last is the 2nd_last fully connected layer in your network and nb_classes is the number of classes in your new dataset.\n",
    "#shape = (fc_2nd_last.get_shape().as_list()[-1], nb_classes)\n",
    "#fc_last_W = tf.Variable(tf.truncated_normal(shape, stddev=1e-2))\n",
    "#fc_last_b = tf.Variable(tf.zeros(nb_classes))\n",
    "#logits = tf.nn.xw_plus_b(fc_2nd_last, fc_last_W, fc_last_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer learning Imports\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras import __version__\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "IM_WIDTH, IM_HEIGHT = 299, 299 #fixed size for InceptionV3\n",
    "NB_EPOCHS = 3\n",
    "BAT_SIZE = 32\n",
    "FC_SIZE = 1024\n",
    "NB_IV3_LAYERS_TO_FREEZE = 172"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Helper Functions ###\n",
    "def get_nb_files(directory):\n",
    "    \"\"\"Get number of files by searching directory recursively\"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        return 0\n",
    "    cnt = 0\n",
    "    \n",
    "    for r, dirs, files in os.walk(directory):\n",
    "        for dr in dirs:\n",
    "            cnt += len(glob.glob(os.path.join(r, dr + \"/*\")))\n",
    "    return cnt\n",
    "\n",
    "def setup_to_transfer_learn(model, base_model):\n",
    "    \"\"\"Freeze all layers and compile the model\"\"\"\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "def add_new_last_layer(base_model, nb_classes):\n",
    "    \"\"\"Add last layer to the convnet\n",
    "    Args:\n",
    "    base_model: keras model excluding top\n",
    "    nb_classes: # of classes\n",
    "    Returns:\n",
    "    new keras model with last layer\n",
    "    \"\"\"\n",
    "    \n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(FC_SIZE, activation='relu')(x) #new FC layer, random init\n",
    "    predictions = Dense(nb_classes, activation='softmax')(x) #new softmax layer\n",
    "    model = Model(input=base_model.input, output=predictions)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def setup_to_finetune(model):\n",
    "    \"\"\"Freeze the bottom NB_IV3_LAYERS and retrain the remaining top layers.\n",
    "    note: NB_IV3_LAYERS corresponds to the top 2 inception blocks in the inceptionv3 arch\n",
    "    Args:\n",
    "    model: keras model\n",
    "    \"\"\"\n",
    "    for layer in model.layers[:NB_IV3_LAYERS_TO_FREEZE]:\n",
    "        layer.trainable = False\n",
    "    for layer in model.layers[NB_IV3_LAYERS_TO_FREEZE:]:\n",
    "        layer.trainable = True\n",
    "    model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use transfer learning and fine-tuning to train a network on a new dataset ###\n",
    "\n",
    "nb_train_samples = get_nb_files(args.train_dir)\n",
    "nb_val_samples = get_nb_files(args.val_dir)\n",
    "\n",
    "nb_classes = 4\n",
    "nb_epoch = 2\n",
    "batch_size = 2\n",
    "\n",
    "# data prep\n",
    "train_datagen =  ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    args.train_dir,\n",
    "    target_size=(IM_WIDTH, IM_HEIGHT),\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    args.val_dir,\n",
    "    target_size=(IM_WIDTH, IM_HEIGHT),\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "# setup model\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False) #include_top=False excludes final FC layer\n",
    "model = add_new_last_layer(base_model, nb_classes)\n",
    "\n",
    "# transfer learning\n",
    "setup_to_transfer_learn(model, base_model)\n",
    "\n",
    "history_tl = model.fit_generator(\n",
    "                            train_generator,\n",
    "                            nb_epoch=nb_epoch,\n",
    "                            samples_per_epoch=nb_train_samples,\n",
    "                            validation_data=validation_generator,\n",
    "                            nb_val_samples=nb_val_samples,\n",
    "                            class_weight='auto')\n",
    "\n",
    "# fine-tuning\n",
    "setup_to_finetune(model)\n",
    "\n",
    "history_ft = model.fit_generator(\n",
    "                            train_generator,\n",
    "                            samples_per_epoch=nb_train_samples,\n",
    "                            nb_epoch=nb_epoch,\n",
    "                            validation_data=validation_generator,\n",
    "                            nb_val_samples=nb_val_samples,\n",
    "                            class_weight='auto')\n",
    "\n",
    "model.save(args.output_model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'r.')\n",
    "plt.plot(epochs, val_acc, 'r')\n",
    "plt.title('Training and validation accuracy')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'r.')\n",
    "plt.plot(epochs, val_loss, 'r-')\n",
    "plt.title('Training and validation loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
